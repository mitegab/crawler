# Tech News Crawler - Current Status

## ‚úÖ Completed

### Backend Setup
- ‚úÖ Python backend configured with Appwrite integration
- ‚úÖ Environment variables configured (.env)
- ‚úÖ Database connection established
- ‚úÖ Article storage tested and working
- ‚úÖ 6 test articles created in database

### Frontend Setup
- ‚úÖ Vue.js frontend running on http://localhost:5173
- ‚úÖ Appwrite Web SDK configured
- ‚úÖ Environment variables configured
- ‚úÖ Connection to Appwrite verified

### Database Configuration
- ‚úÖ Database ID: `68db0591001cd667383f`
- ‚úÖ Collection: `articles` with 9 attributes
- ‚úÖ Schema: `title`, `title_am`, `url`, `summary`, `summary_am`, `source`, `image_url`, `published_date`, `category`
- ‚úÖ Test articles successfully stored

### Python 3.13 Compatibility
- ‚úÖ Translation feature disabled (googletrans incompatibility)
- ‚úÖ Scraping and storage working with English-only articles

## ‚ö†Ô∏è Known Issues

### 1. Web Scrapers Not Working
The scrapers are returning 0 articles because:
- Website HTML structures have changed
- Selectors need to be updated
- All 4 scrapers (TechCrunch, The Verge, Ars Technica, Wired) affected

### 2. Collection Permissions
- Frontend may not be able to read articles without proper permissions
- **Manual fix required:**
  1. Go to [Appwrite Console](https://cloud.appwrite.io/console/project/68daf772003b1b1f5c78)
  2. Navigate to: Databases ‚Üí Select database (ID: 68db0591001cd667383f) ‚Üí articles ‚Üí Settings ‚Üí Permissions
  3. Click "Add Role"
  4. Select "Any" and enable "Read" permission
  5. Click "Update"

### 3. Translation Temporarily Disabled
- Python 3.13 removed `cgi` module
- googletrans depends on httpx 0.13.3 which requires cgi
- **Current workaround:** Translation returns None
- **Future solutions:**
  - Use Python 3.11 or 3.12 environment
  - Switch to `deep-translator` library
  - Use Azure Translator or OpenAI API
  - Deploy translation as separate microservice

## üìã Next Steps

### Immediate (To Make It Work)
1. **Set collection permissions** (see instructions above)
2. **Update web scrapers** to work with current website structures:
   - Inspect each website's HTML
   - Update CSS selectors
   - Test each scraper individually
3. **Verify frontend displays articles**
   - Open http://localhost:5173
   - Check if articles load
   - Test filtering/sorting if implemented

### Short Term
1. **Fix all 4 scrapers:**
   - TechCrunch
   - The Verge
   - Ars Technica
   - Wired
2. **Add error handling:**
   - Network timeouts
   - Invalid article data
   - Rate limiting
3. **Set up automated scraping:**
   - Cron job every 6 hours
   - Or deploy as Appwrite Function

### Medium Term
1. **Resolve translation:**
   - Test `deep-translator` library
   - Or use external translation API
2. **Implement frontend features:**
   - Article search
   - Category filtering
   - Date sorting
   - Language toggle
3. **Add monitoring:**
   - Scraper success rates
   - Database growth
   - Error tracking

### Long Term
1. **Deploy backend:**
   - Containerize with Docker
   - Deploy to cloud (Appwrite Functions, AWS Lambda, etc.)
2. **Deploy frontend:**
   - Build for production
   - Deploy to Vercel/Netlify/Cloudflare Pages
3. **Add features:**
   - RSS feeds
   - Email notifications
   - User accounts
   - Bookmarks

## üîß Useful Commands

### Backend
```bash
# Run crawler (from project root)
cd backend && /bin/python main.py

# Run with output logging
cd backend && /bin/python -u main.py | tee crawler.log
```

### Frontend
```bash
# Run dev server (from frontend directory)
cd /home/ethiopique/Downloads/os/tech-news-frontend
npm run dev
```

### Database
```bash
# List all articles
/bin/python << 'EOF'
import sys
sys.path.insert(0, 'backend')
from backend.config import settings
from backend.services.appwrite_manager import AppwriteManager

am = AppwriteManager()
articles = am.databases.list_documents(
    database_id=am.database_id,
    collection_id=am.articles_collection_id
)
print(f"Total articles: {articles['total']}")
for doc in articles['documents']:
    print(f"- {doc['title']}")
EOF
```

## üìÅ Project Structure

```
/home/ethiopique/Downloads/os/crawler/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ config/          # Configuration and settings
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/        # Web scrapers (needs fixing)
‚îÇ   ‚îú‚îÄ‚îÄ services/        # Appwrite, translation services
‚îÇ   ‚îú‚îÄ‚îÄ translators/     # Translation logic
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Logger and utilities
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # Entry point
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ VUE_FRONTEND_SETUP.md  # Frontend setup guide
‚îú‚îÄ‚îÄ .env                 # Backend environment variables
‚îî‚îÄ‚îÄ README.md

/home/ethiopique/Downloads/os/tech-news-frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/      # Vue components (to be created)
‚îÇ   ‚îú‚îÄ‚îÄ composables/     # Appwrite logic (to be created)
‚îÇ   ‚îî‚îÄ‚îÄ App.vue
‚îú‚îÄ‚îÄ .env                 # Frontend environment variables
‚îî‚îÄ‚îÄ package.json
```

## üîë Appwrite Configuration

- **Project ID:** `68daf772003b1b1f5c78`
- **Endpoint:** `https://nyc.cloud.appwrite.io/v1`
- **Database ID:** `68db0591001cd667383f`
- **Collection ID:** `articles`
- **Storage Bucket:** `article-images` (to be created)

## üìä Database Schema

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| title | String(500) | ‚úì | Article title in English |
| title_am | String(500) | - | Article title in Amharic |
| url | URL | ‚úì | Original article URL |
| summary | String(5000) | - | Article summary in English |
| summary_am | String(5000) | - | Article summary in Amharic |
| source | String(200) | ‚úì | Source name (TechCrunch, etc.) |
| image_url | URL | - | Featured image URL |
| published_date | DateTime | - | Original publication date |
| category | String(100) | - | Article category/topic |

## üéØ Success Criteria

- [x] Backend can connect to Appwrite
- [x] Backend can save articles to database
- [x] Frontend can connect to Appwrite
- [ ] Frontend can display articles
- [ ] Scrapers successfully fetch articles
- [ ] Translation working (or postponed)
- [ ] Automated scraping scheduled
